{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d325ca",
   "metadata": {},
   "source": [
    "# AIDI 1002: Machine Learning Programming — Assignment - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9492e9",
   "metadata": {},
   "source": [
    "1. Consider the dataset ‘noisy_data.csv’ and apply the following pre-processing techniques and obtain the clean dataset.\n",
    "\n",
    " - Handling missing values by imputation\n",
    " - Apply Normality tests to numerical columns and state the hypothesis clearly and comment on the normality of the data\n",
    " - Apply encodings for categorical variable and scale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f72ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18300422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Age</th>\n",
       "      <th>Income</th>\n",
       "      <th>Online Shopper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India</td>\n",
       "      <td>49.0</td>\n",
       "      <td>86400.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>32.0</td>\n",
       "      <td>57600.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USA</td>\n",
       "      <td>35.0</td>\n",
       "      <td>64800.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>43.0</td>\n",
       "      <td>73200.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USA</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India</td>\n",
       "      <td>40.0</td>\n",
       "      <td>69600.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62400.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>India</td>\n",
       "      <td>53.0</td>\n",
       "      <td>94800.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>USA</td>\n",
       "      <td>55.0</td>\n",
       "      <td>99600.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>India</td>\n",
       "      <td>42.0</td>\n",
       "      <td>80400.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Region   Age   Income Online Shopper\n",
       "0   India  49.0  86400.0             No\n",
       "1  Brazil  32.0  57600.0            Yes\n",
       "2     USA  35.0  64800.0             No\n",
       "3  Brazil  43.0  73200.0             No\n",
       "4     USA  45.0      NaN            Yes\n",
       "5   India  40.0  69600.0            Yes\n",
       "6  Brazil   NaN  62400.0             No\n",
       "7   India  53.0  94800.0            Yes\n",
       "8     USA  55.0  99600.0             No\n",
       "9   India  42.0  80400.0            Yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"noisy_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cd66ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.iloc[:,:-1].values\n",
    "target = df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73daeb2",
   "metadata": {},
   "source": [
    "## Handling Missing Values By Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "733b6465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['India', 49.0, 86400.0],\n",
       "       ['Brazil', 32.0, 57600.0],\n",
       "       ['USA', 35.0, 64800.0],\n",
       "       ['Brazil', 43.0, 73200.0],\n",
       "       ['USA', 45.0, 76533.33333333333],\n",
       "       ['India', 40.0, 69600.0],\n",
       "       ['Brazil', 43.77777777777778, 62400.0],\n",
       "       ['India', 53.0, 94800.0],\n",
       "       ['USA', 55.0, 99600.0],\n",
       "       ['India', 42.0, 80400.0]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputa = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "\n",
    "imputa.fit(features[:, 1:3])\n",
    "features[:, 1:3] = imputa.transform(features[:, 1:3])\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b361c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [86400.0 57600.0 64800.0 73200.0 76533.33333333333 69600.0 62400.0 94800.0\n",
      " 99600.0 80400.0]\n",
      "2 [49.0 32.0 35.0 43.0 45.0 40.0 43.77777777777778 53.0 55.0 42.0]\n"
     ]
    }
   ],
   "source": [
    "print('1',features[:,-1])\n",
    "print('2',features[:,-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbff81",
   "metadata": {},
   "source": [
    "## Normality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba00440f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable 1 is normally distributed.\n",
      "Variable 2 is normally distributed.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "feature_1 = features[:,-1]\n",
    "feature_2 = features[:,-2]\n",
    "\n",
    "stat1, p_value1 = shapiro(feature_1)\n",
    "stat2, p_value2 = shapiro(feature_2)\n",
    "\n",
    "if p_value1 > 0.05:\n",
    "    print(\"Variable 1 is normally distributed.\")\n",
    "else:\n",
    "    print(\"Variable 1 is not normally distributed.\")\n",
    "\n",
    "if p_value2 > 0.05:\n",
    "    print(\"Variable 2 is normally distributed.\")\n",
    "else:\n",
    "    print(\"Variable 2 is not normally distributed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e597e",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7718a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, 49.0, 86400.0],\n",
       "       [1.0, 0.0, 0.0, 32.0, 57600.0],\n",
       "       [0.0, 0.0, 1.0, 35.0, 64800.0],\n",
       "       [1.0, 0.0, 0.0, 43.0, 73200.0],\n",
       "       [0.0, 0.0, 1.0, 45.0, 76533.33333333333],\n",
       "       [0.0, 1.0, 0.0, 40.0, 69600.0],\n",
       "       [1.0, 0.0, 0.0, 43.77777777777778, 62400.0],\n",
       "       [0.0, 1.0, 0.0, 53.0, 94800.0],\n",
       "       [0.0, 0.0, 1.0, 55.0, 99600.0],\n",
       "       [0.0, 1.0, 0.0, 42.0, 80400.0]], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder= 'passthrough')\n",
    "features = np.array(ct.fit_transform(features))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2be48df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "LE = LabelEncoder()\n",
    "target = LE.fit_transform(target)\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97099a0",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f36ceee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.65465367,  1.22474487, -0.65465367,  0.75887436,  0.74947325],\n",
       "       [ 1.52752523, -0.81649658, -0.65465367, -1.71150388, -1.43817841],\n",
       "       [-0.65465367, -0.81649658,  1.52752523, -1.27555478, -0.89126549],\n",
       "       [ 1.52752523, -0.81649658, -0.65465367, -0.11302384, -0.25320042],\n",
       "       [-0.65465367, -0.81649658,  1.52752523,  0.17760889,  0.        ],\n",
       "       [-0.65465367,  1.22474487, -0.65465367, -0.54897294, -0.52665688],\n",
       "       [ 1.52752523, -0.81649658, -0.65465367,  0.        , -1.0735698 ],\n",
       "       [-0.65465367,  1.22474487, -0.65465367,  1.34013983,  1.38753832],\n",
       "       [-0.65465367, -0.81649658,  1.52752523,  1.63077256,  1.75214693],\n",
       "       [-0.65465367,  1.22474487, -0.65465367, -0.25834021,  0.29371249]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab9daa3",
   "metadata": {},
   "source": [
    "2. Consider the text present in the file ‘wiki.txt’ and Answer the following questions :\n",
    "\n",
    "    \n",
    " - Write a program to convert following text into tokens with two tokenization methods such as ‘RegexpTokenizer()’ and \n",
    "  ‘word_tokenize()’ from NLTK library. (Note :The tokens should not have stop words and punctuation symbols. Feel free to \n",
    "   decide about the correct list of stop words; e.g., negative words (don’t) could be important for you. Execute both methods\n",
    "   of tokenization along with your code of removing stop words and punctuation.)\n",
    " - Write a regular expression to extract all the year mentions in the ‘wiki.txt’ file.\n",
    " - State the differences observed in the output of tokenization methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c37243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\aksha\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55b4f697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7449e5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens using RegexpTokenizer:\n",
      "['history', 'NLP', 'generally', 'started', '1950s', 'although', 'work', 'found', 'earlier', 'periods', '1950', 'Alan', 'Turing', 'published', 'article', 'titled', 'Computing', 'Machinery', 'Intelligence', 'proposed', 'called', 'Turing', 'test', 'criterion', 'intelligence', 'Georgetown', 'experiment', '1954', 'involved', 'fully', 'automatic', 'translation', 'sixty', 'Russian', 'sentences', 'English', 'authors', 'claimed', 'within', 'three', 'five', 'years', 'machine', 'translation', 'would', 'solved', 'problem', '2', 'However', 'real', 'progress', 'much', 'slower', 'ALPAC', 'report', '1966', 'found', 'ten', 'year', 'long', 'research', 'failed', 'fulfill', 'expectations', 'funding', 'machine', 'translation', 'dramatically', 'reduced', 'Little', 'research', 'machine', 'translation', 'conducted', 'late', '1980s', 'first', 'statistical', 'machine', 'translation', 'systems', 'developed', 'notably', 'successful', 'NLP', 'systems', 'developed', '1960s', 'SHRDLU', 'natural', 'language', 'system', 'working', 'restricted', 'blocks', 'worlds', 'restricted', 'vocabularies', 'ELIZA', 'simulation', 'Rogerian', 'psychotherapist', 'written', 'Joseph', 'Weizenbaum', '1964', '1966', 'Using', 'almost', 'information', 'human', 'thought', 'emotion', 'ELIZA', 'sometimes', 'provided', 'startlingly', 'human', 'like', 'interaction', 'patient', 'exceeded', 'small', 'knowledge', 'base', 'ELIZA', 'might', 'provide', 'generic', 'response', 'example', 'responding', 'head', 'hurts', 'say', 'head', 'hurts']\n",
      "\n",
      "Tokens using word_tokenize:\n",
      "['history', 'NLP', 'generally', 'started', '1950s', 'although', 'work', 'found', 'earlier', 'periods', '1950', 'Alan', 'Turing', 'published', 'article', 'titled', '``', 'Computing', 'Machinery', 'Intelligence', \"''\", 'proposed', 'called', 'Turing', 'test', 'criterion', 'intelligence', 'Georgetown', 'experiment', '1954', 'involved', 'fully', 'automatic', 'translation', 'sixty', 'Russian', 'sentences', 'English', 'authors', 'claimed', 'within', 'three', 'five', 'years', 'machine', 'translation', 'would', 'solved', 'problem', '2', 'However', 'real', 'progress', 'much', 'slower', 'ALPAC', 'report', '1966', 'found', 'ten-year-long', 'research', 'failed', 'fulfill', 'expectations', 'funding', 'machine', 'translation', 'dramatically', 'reduced', 'Little', 'research', 'machine', 'translation', 'conducted', 'late', '1980s', 'first', 'statistical', 'machine', 'translation', 'systems', 'developed', 'notably', 'successful', 'NLP', 'systems', 'developed', '1960s', 'SHRDLU', 'natural-language', 'system', 'working', 'restricted', '``', 'blocks', 'worlds', \"''\", 'restricted', 'vocabularies', 'ELIZA', 'simulation', 'Rogerian', 'psychotherapist', 'written', 'Joseph', 'Weizenbaum', '1964', '1966', 'Using', 'almost', 'information', 'human', 'thought', 'emotion', 'ELIZA', 'sometimes', 'provided', 'startlingly', 'human-like', 'interaction', '``', 'patient', \"''\", 'exceeded', 'small', 'knowledge', 'base', 'ELIZA', 'might', 'provide', 'generic', 'response', 'example', 'responding', '``', 'head', 'hurts', \"''\", '``', 'say', 'head', 'hurts', '``']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from a text file\n",
    "with open('wiki.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "# Tokenization using RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens_regexp = tokenizer.tokenize(data)\n",
    "\n",
    "# Tokenization using word_tokenize\n",
    "tokens_word = word_tokenize(data)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_regexp = [token for token in tokens_regexp if token.lower() not in stop_words]\n",
    "tokens_word = [token for token in tokens_word if token.lower() not in stop_words]\n",
    "\n",
    "# Remove punctuation symbols\n",
    "tokens_regexp = [token for token in tokens_regexp if token not in string.punctuation]\n",
    "tokens_word = [token for token in tokens_word if token not in string.punctuation]\n",
    "\n",
    "# Print the tokens\n",
    "print(\"Tokens using RegexpTokenizer:\")\n",
    "print(tokens_regexp)\n",
    "\n",
    "print(\"\\nTokens using word_tokenize:\")\n",
    "print(tokens_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dde55220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1950', '1950', '1954', '1966', '1980', '1960', '1964', '1966']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open('wiki.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "pattern = '\\d{4}'\n",
    "dates = re.findall(pattern, data)\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d488a",
   "metadata": {},
   "source": [
    "Observations\n",
    "\n",
    "1. Word tokenization includes special letters and punctuation, while ordinary expression tokenization does not. \n",
    "2. Using a regular expression technique, the tokens are created using the NLTK word tokenizer's default tokenizer and divided depending on word boundaries.\n",
    "3. The word tokenizer found 143 tokens in the wiki.txt dataset, compared to 137 tokens found by the regular expression tokenizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5179c0b",
   "metadata": {},
   "source": [
    "3. Consider this dataset from kaggle. (Download the dataset from following link : https://www.kaggle.com/dansbecker/\n",
    "melbourne-housing-snapshot/home) and answer the following questions :\n",
    " - Apply the feature selection techniques over the melbourne-housing -dataset namely (20 points):\n",
    "    ∗ Correlation\n",
    "    ∗ Chi-Square\n",
    "    ∗ Mutual-Information\n",
    "    ∗ Random Forest feature importance\n",
    " - Compare the importance of selected features using bar chart (10 points).\n",
    " - Comment on the results obtained from various feature selection techniques and which is the best and worst feature selection      technique on the given dataset (10 points). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a1b7609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Suburb</th>\n",
       "      <th>Address</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Type</th>\n",
       "      <th>Price</th>\n",
       "      <th>Method</th>\n",
       "      <th>SellerG</th>\n",
       "      <th>Date</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>...</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Car</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>BuildingArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>CouncilArea</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>85 Turner St</td>\n",
       "      <td>2</td>\n",
       "      <td>h</td>\n",
       "      <td>1480000</td>\n",
       "      <td>S</td>\n",
       "      <td>Biggin</td>\n",
       "      <td>03-12-2016</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3067</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.7996</td>\n",
       "      <td>144.9984</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>25 Bloomburg St</td>\n",
       "      <td>2</td>\n",
       "      <td>h</td>\n",
       "      <td>1035000</td>\n",
       "      <td>S</td>\n",
       "      <td>Biggin</td>\n",
       "      <td>04-02-2016</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3067</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.8079</td>\n",
       "      <td>144.9934</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>5 Charles St</td>\n",
       "      <td>3</td>\n",
       "      <td>h</td>\n",
       "      <td>1465000</td>\n",
       "      <td>SP</td>\n",
       "      <td>Biggin</td>\n",
       "      <td>04-03-2017</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3067</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>134</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.8093</td>\n",
       "      <td>144.9944</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>40 Federation La</td>\n",
       "      <td>3</td>\n",
       "      <td>h</td>\n",
       "      <td>850000</td>\n",
       "      <td>PI</td>\n",
       "      <td>Biggin</td>\n",
       "      <td>04-03-2017</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3067</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.7969</td>\n",
       "      <td>144.9969</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>55a Park St</td>\n",
       "      <td>4</td>\n",
       "      <td>h</td>\n",
       "      <td>1600000</td>\n",
       "      <td>VB</td>\n",
       "      <td>Nelson</td>\n",
       "      <td>04-06-2016</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3067</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120</td>\n",
       "      <td>142.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.8072</td>\n",
       "      <td>144.9941</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Suburb           Address  Rooms Type    Price Method SellerG  \\\n",
       "0  Abbotsford      85 Turner St      2    h  1480000      S  Biggin   \n",
       "1  Abbotsford   25 Bloomburg St      2    h  1035000      S  Biggin   \n",
       "2  Abbotsford      5 Charles St      3    h  1465000     SP  Biggin   \n",
       "3  Abbotsford  40 Federation La      3    h   850000     PI  Biggin   \n",
       "4  Abbotsford       55a Park St      4    h  1600000     VB  Nelson   \n",
       "\n",
       "         Date  Distance  Postcode  ...  Bathroom  Car  Landsize  BuildingArea  \\\n",
       "0  03-12-2016       2.5      3067  ...         1  1.0       202           NaN   \n",
       "1  04-02-2016       2.5      3067  ...         1  0.0       156          79.0   \n",
       "2  04-03-2017       2.5      3067  ...         2  0.0       134         150.0   \n",
       "3  04-03-2017       2.5      3067  ...         2  1.0        94           NaN   \n",
       "4  04-06-2016       2.5      3067  ...         1  2.0       120         142.0   \n",
       "\n",
       "   YearBuilt  CouncilArea Lattitude  Longtitude             Regionname  \\\n",
       "0        NaN        Yarra  -37.7996    144.9984  Northern Metropolitan   \n",
       "1     1900.0        Yarra  -37.8079    144.9934  Northern Metropolitan   \n",
       "2     1900.0        Yarra  -37.8093    144.9944  Northern Metropolitan   \n",
       "3        NaN        Yarra  -37.7969    144.9969  Northern Metropolitan   \n",
       "4     2014.0        Yarra  -37.8072    144.9941  Northern Metropolitan   \n",
       "\n",
       "  Propertycount  \n",
       "0          4019  \n",
       "1          4019  \n",
       "2          4019  \n",
       "3          4019  \n",
       "4          4019  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"melb_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce61fbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13580 entries, 0 to 13579\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Suburb         13580 non-null  object \n",
      " 1   Address        13580 non-null  object \n",
      " 2   Rooms          13580 non-null  int64  \n",
      " 3   Type           13580 non-null  object \n",
      " 4   Price          13580 non-null  int64  \n",
      " 5   Method         13580 non-null  object \n",
      " 6   SellerG        13580 non-null  object \n",
      " 7   Date           13580 non-null  object \n",
      " 8   Distance       13580 non-null  float64\n",
      " 9   Postcode       13580 non-null  int64  \n",
      " 10  Bedroom2       13580 non-null  int64  \n",
      " 11  Bathroom       13580 non-null  int64  \n",
      " 12  Car            13518 non-null  float64\n",
      " 13  Landsize       13580 non-null  int64  \n",
      " 14  BuildingArea   7130 non-null   float64\n",
      " 15  YearBuilt      8205 non-null   float64\n",
      " 16  CouncilArea    12211 non-null  object \n",
      " 17  Lattitude      13580 non-null  float64\n",
      " 18  Longtitude     13580 non-null  float64\n",
      " 19  Regionname     13580 non-null  object \n",
      " 20  Propertycount  13580 non-null  int64  \n",
      "dtypes: float64(6), int64(7), object(8)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eee72816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "# Remove rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "encoder = LabelEncoder()\n",
    "df['Suburb'] = encoder.fit_transform(df['Suburb'])\n",
    "df['Type'] = encoder.fit_transform(df['Type'])\n",
    "df['Method'] = encoder.fit_transform(df['Method'])\n",
    "df['Regionname'] = encoder.fit_transform(df['Regionname'])\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X = df.drop(['Price','Address','SellerG', 'Date','CouncilArea', 'Postcode'], axis=1)\n",
    "y = df['Price']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89b0c981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation features:\n",
      "['Price', 'Rooms', 'BuildingArea', 'Bedroom2', 'Bathroom', 'Type', 'YearBuilt', 'Car', 'Lattitude', 'Longtitude']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksha\\AppData\\Local\\Temp\\ipykernel_9388\\2939289232.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  correlation_matrix = df.corr()\n"
     ]
    }
   ],
   "source": [
    "# Feature selection using correlation\n",
    "\n",
    "# Exclude non-numeric columns from the correlation matrix\n",
    "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "correlation_matrix = df[numeric_columns].corr()\n",
    "\n",
    "# Select features based on correlation with 'Price'\n",
    "correlation_features = correlation_matrix.abs()['Price'].nlargest(10).index\n",
    "df_correlation = df[correlation_features]\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Correlation features:\")\n",
    "print(df_correlation.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2734aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chi-square features:\n",
      "['Suburb', 'Rooms', 'Type', 'Method', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Regionname', 'Propertycount']\n",
      "Scores [ 283.2024679   229.65192647 1919.26528772  297.16731238  116.33144932\n",
      "  113.08855908  360.31473464  107.64960098   66.69154453   70.46453266\n",
      "    5.02193561   41.03981202   41.61254177  236.09459749  193.38627623]\n"
     ]
    }
   ],
   "source": [
    "# Feature selection using chi-square\n",
    "chi2_selector = SelectKBest(score_func=chi2, k=10)\n",
    "X_chi2 = chi2_selector.fit_transform(x, y)\n",
    "\n",
    "print(\"\\nChi-square features:\")\n",
    "print(X.columns[chi2_selector.get_support()].tolist())\n",
    "print('Scores',chi2_selector.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b089a1c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m sorted_feature_names \u001b[38;5;241m=\u001b[39m feature_names[sorted_indices]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create the figure and axes\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Plot the sorted importance scores in a bar chart\u001b[39;00m\n\u001b[0;32m     12\u001b[0m ax\u001b[38;5;241m.\u001b[39mbar(sorted_feature_names, sorted_importance_scores)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Sort the importance scores and feature names in descending order\n",
    "\n",
    "feature_names = X.columns\n",
    "sorted_indices = np.argsort(chi2_selector.scores_)[::-1]\n",
    "sorted_importance_scores = chi2_selector.scores_[sorted_indices]\n",
    "sorted_feature_names = feature_names[sorted_indices]\n",
    " \n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the sorted importance scores in a bar chart\n",
    "ax.bar(sorted_feature_names, sorted_importance_scores)\n",
    "\n",
    "# Rotate x-axis labels for better readability if needed\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(['Importance Score'])\n",
    "\n",
    "\n",
    "# Set the x-axis label\n",
    "ax.set_xlabel('Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9462193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection using mutual information\n",
    "mi_selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_mi = mi_selector.fit_transform(X, y)\n",
    "\n",
    "print('Scores: ', mi_selector.scores_)\n",
    "print(\"\\nMutual information features:\")\n",
    "print(X.columns[mi_selector.get_support()].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the importance scores and feature names in descending order\n",
    "\n",
    "sorted_indices = np.argsort(mi_selector.scores_)[::-1]\n",
    "sorted_importance_scores = mi_selector.scores_[sorted_indices]\n",
    "sorted_feature_names = feature_names[sorted_indices]\n",
    " \n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the sorted importance scores in a bar chart\n",
    "ax.bar(sorted_feature_names, sorted_importance_scores)\n",
    "\n",
    "# Rotate x-axis labels for better readability if needed\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.legend(['Importance Score'])\n",
    "\n",
    "# Set the x-axis label\n",
    "ax.set_xlabel('Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier( n_estimators=50)\n",
    "\n",
    "model = clf.fit(X,y)\n",
    "feat_importances = pd.DataFrame(model.feature_importances_, index=X.columns, columns=[\"Importance\"])\n",
    "feat_importances.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "print(\"\\nRandom forest feature importance features:\\n\")\n",
    "print(feat_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "feat_importances.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "top_features = feat_importances.head(10)\n",
    "top_features.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60938dc7",
   "metadata": {},
   "source": [
    "Here, the bar graph shows the 10 most important features that contribute to the target variable, and is observed with the \n",
    "help of Random forest classifier. X-axis denotes the features and y-axis denotes the scores. Its noticed that Lattitude, Longitude, BuildingArea, and landsize has the highest importance scores. Method, Car, Suburb, and Propertycount being the least out of best 10 features. In addition, Bedroom2, Rooms, Bathroom, Regionname, Type having poor importance scores than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf00356",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "As a concluding observation, when the dataset's structure and techniques are taken into account, Random forest feature importance and mutual information tend to be more dependable and adaptable in capturing various kinds of correlations between features and the target variable. Only linear relationships may be represented using the correlation matrix, and categorical variables can only be studied using Chi-square."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
